{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "from TVL2 import *\n",
    "from TVICE import logerfc, logerf2\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV - Iterated Conditional Expectation\n",
    "\n",
    "Ce notebook porte sur le débruitage d'images grâce à l'algorithme *Iterated Conditional Expectation*. Il est inspiré de l'article\n",
    "\n",
    "\\[1\\] [Louchet, C.,  Moisan, L., *Total variation denoising using iterated conditional expectation*. In 2014 22nd European Signal Processing Conference (EUSIPCO) (pp. 1592-1596). 2014](https://hal.archives-ouvertes.fr/hal-01214735)\n",
    "\n",
    "Voir le sujet PDF associé pour plus de détails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 20   # TV regularization parameter \n",
    "sig = 0.07  # noise standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image creation\n",
    "n = 500    # start with small images for your experimentations\n",
    "i = 100\n",
    "u = plt.imread('./img/simpson512.png')\n",
    "u = u[:,:,1]\n",
    "u = u[i:i+n,i:i+n]\n",
    "nr,nc = u.shape\n",
    "\n",
    "# add noise\n",
    "ub = u + sig*np.random.randn(nr,nc)\n",
    "noise = np.random.rand(nr,nc)\n",
    "\n",
    "# TV-MAP\n",
    "u_tvmap = chambolle_pock_prox_TV1(ub,sig**2*lambd,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(u, col_idx):\n",
    "    a = np.zeros((u.shape[0]))\n",
    "    b = np.zeros((u.shape[0]))\n",
    "    c = np.zeros((u.shape[0]))\n",
    "    d = np.zeros((u.shape[0]))\n",
    "    \n",
    "    n = u.shape[0]\n",
    "    m = u.shape[1]\n",
    "    \n",
    "    for i in range(u.shape[0]):\n",
    "    \n",
    "        current = u[i, col_idx]\n",
    "        neighbors = np.zeros((4))\n",
    "        \n",
    "        neighbors[0] = u[(i - 1) % n, col_idx]\n",
    "        neighbors[1] = u[(i + 1) % n, col_idx]\n",
    "        neighbors[2] = u[i, (col_idx - 1) % m]\n",
    "        neighbors[3] = u[i, (col_idx + 1) % m]\n",
    "        \n",
    "        neighbors.sort()\n",
    "        a[i], b[i], c[i], d[i] = neighbors\n",
    "        \n",
    "    return a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvice(u0,sig,lambd,n_iter=50): \n",
    "    \"\"\"\n",
    "    Computes the u_ice estimate\n",
    "    \n",
    "    Parameters:\n",
    "    u0: Noisy Image\n",
    "    sig: sigma parameter\n",
    "    lambd: lambda parameter\n",
    "    n_iter: # of iterations to run\n",
    "    \n",
    "    Returns:\n",
    "    Approximation of u_ice the denoised image\n",
    "    \"\"\"\n",
    "    u = np.copy(u0)\n",
    "    \n",
    "    history = list()\n",
    "    \n",
    "    for iter_idx in range(1, n_iter + 1):\n",
    "        for i in range(u.shape[1]):\n",
    "            t = u0[:, i]\n",
    "            a, b, c, d = get_vectors(u, i)\n",
    "            \n",
    "            X_2 = logerfc((t - a + 4*lambd*sig**2)/(sig*np.sqrt(2)))+\\\n",
    "                            (2*lambd*(2*(t + 2*lambd*sig**2) - a - b)) \n",
    "            \n",
    "            X_1 = logerf2((a - t - 2*lambd*sig**2)/(sig*np.sqrt(2)),\n",
    "                            (b - t - 2*lambd*sig**2)/(sig*np.sqrt(2)))+\\\n",
    "                            lambd*(2*(t - b) + 2*lambd*sig**2)\n",
    "            \n",
    "            X0 = logerf2((b-t)/(sig*np.sqrt(2)), (c-t)/(sig*np.sqrt(2)))\n",
    "            \n",
    "            X1 = logerf2((c - t + 2*lambd*sig**2)/(sig*np.sqrt(2)),\n",
    "                            (d - t + 2*lambd*sig**2)/(sig*np.sqrt(2)))+\\\n",
    "                            lambd*(2*(c - t) + 2*lambd*sig**2)\n",
    "            \n",
    "            X2 = logerfc((d - t + 4*lambd*sig**2)/(sig*np.sqrt(2)))+\\\n",
    "                            (2*lambd*(c + d - 2*(t - 2*lambd*sig**2))) \n",
    "            \n",
    "            XS = np.array([X_2, X_1, X0, X1, X2])\n",
    "            M = np.max(XS, axis=0)\n",
    "            X_2, X_1, X0, X1, X2 = np.exp(XS - M)\n",
    "            \n",
    "            ratio = (2*X_2 + X_1 - X1 - 2*X2)/(X_2 + X_1 + X0 + X1 + X2)\n",
    "            u[:, i] = u0[:, i] + 2*lambd*sig**2*ratio\n",
    "        history.append(np.copy(u))\n",
    "    return u, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## denoise ub using the TV-ICE scheme\n",
    "out, history = tvice(ub, sig, lambd, 100)\n",
    "\n",
    "## display\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "axes[0,0].imshow(u,cmap='gray')\n",
    "axes[0,0].set_title('original image')\n",
    "axes[0,1].imshow(ub,cmap='gray')\n",
    "axes[0,1].set_title('noisy image')\n",
    "axes[1,0].imshow(u_tvmap,cmap='gray')\n",
    "axes[1,0].set_title('TV MAP')\n",
    "axes[1,1].imshow(out,cmap='gray')\n",
    "axes[1,1].set_title('Conditional expectation')\n",
    "fig.tight_layout()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 2   # TV regularization parameter \n",
    "sig = 0.07  # noise standard deviation\n",
    "# image creation\n",
    "n = 500    # start with small images for your experimentations\n",
    "i = 100\n",
    "u = plt.imread('./img/simpson512.png')\n",
    "u = u[:,:,1]\n",
    "u = u[i:i+n,i:i+n]\n",
    "nr,nc = u.shape\n",
    "\n",
    "# add noise\n",
    "ub = u + sig*np.random.randn(nr,nc)\n",
    "noise = np.random.rand(nr,nc)\n",
    "\n",
    "# TV-MAP\n",
    "u_tvmap = chambolle_pock_prox_TV1(ub,sig**2*lambd,100)\n",
    "## denoise ub using the TV-ICE scheme\n",
    "out, history = tvice(ub, sig, lambd, 100)\n",
    "\n",
    "## display\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "axes[0,0].imshow(u,cmap='gray')\n",
    "axes[0,0].set_title('original image')\n",
    "axes[0,1].imshow(ub,cmap='gray')\n",
    "axes[0,1].set_title('noisy image')\n",
    "axes[1,0].imshow(u_tvmap,cmap='gray')\n",
    "axes[1,0].set_title('TV MAP')\n",
    "axes[1,1].imshow(out,cmap='gray')\n",
    "axes[1,1].set_title('Conditional expectation')\n",
    "fig.tight_layout()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 50   # TV regularization parameter \n",
    "sig = 0.07  # noise standard deviation\n",
    "# image creation\n",
    "n = 500    # start with small images for your experimentations\n",
    "i = 100\n",
    "u = plt.imread('./img/simpson512.png')\n",
    "u = u[:,:,1]\n",
    "u = u[i:i+n,i:i+n]\n",
    "nr,nc = u.shape\n",
    "\n",
    "# add noise\n",
    "ub = u + sig*np.random.randn(nr,nc)\n",
    "noise = np.random.rand(nr,nc)\n",
    "\n",
    "# TV-MAP\n",
    "u_tvmap = chambolle_pock_prox_TV1(ub,sig**2*lambd,100)\n",
    "## denoise ub using the TV-ICE scheme\n",
    "out, history = tvice(ub, sig, lambd, 100)\n",
    "\n",
    "## display\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "axes[0,0].imshow(u,cmap='gray')\n",
    "axes[0,0].set_title('original image')\n",
    "axes[0,1].imshow(ub,cmap='gray')\n",
    "axes[0,1].set_title('noisy image')\n",
    "axes[1,0].imshow(u_tvmap,cmap='gray')\n",
    "axes[1,0].set_title('TV MAP')\n",
    "axes[1,1].imshow(out,cmap='gray')\n",
    "axes[1,1].set_title('Conditional expectation')\n",
    "fig.tight_layout()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 30  # TV regularization parameter \n",
    "sig = 0.07  # noise standard deviation\n",
    "# image creation\n",
    "n = 500    # start with small images for your experimentations\n",
    "i = 100\n",
    "u = plt.imread('./img/simpson512.png')\n",
    "u = u[:,:,1]\n",
    "u = u[i:i+n,i:i+n]\n",
    "nr,nc = u.shape\n",
    "\n",
    "# add noise\n",
    "ub = u + sig*np.random.randn(nr,nc)\n",
    "noise = np.random.rand(nr,nc)\n",
    "\n",
    "# TV-MAP\n",
    "u_tvmap = chambolle_pock_prox_TV1(ub,sig**2*lambd,100)\n",
    "## denoise ub using the TV-ICE scheme\n",
    "out, history = tvice(ub, sig, lambd, 100)\n",
    "\n",
    "## display\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "axes[0,0].imshow(u,cmap='gray')\n",
    "axes[0,0].set_title('original image')\n",
    "axes[0,1].imshow(ub,cmap='gray')\n",
    "axes[0,1].set_title('noisy image')\n",
    "axes[1,0].imshow(u_tvmap,cmap='gray')\n",
    "axes[1,0].set_title('TV MAP')\n",
    "axes[1,1].imshow(out,cmap='gray')\n",
    "axes[1,1].set_title('Conditional expectation')\n",
    "fig.tight_layout()\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to the PDF questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Loi marginale conditionnelle $\\pi(u(i)|u(i^c))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons l'égalité suivante:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi(u(i)|u(i^c)) &= \\frac{\\pi(u)}{\\pi(u(i^c))} \\\\\n",
    "&= \\frac{\\pi(u)}{\\int_\\mathbb{R} \\pi(x,u(i^c)) dx} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Nous pouvons développer l'intégrale pour obtenir:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi(u(i)|u(i^c)) &= \\frac{e^{- \\frac{1}{2\\sigma^2} |u(i) - u_0(i)|^2 - \\lambda \\sum_{j\\in N(i)} |u(i) - u(j)|}}{Z}\n",
    "\\end{align}\n",
    "$$\n",
    "avec $Z = \\int_\\mathbb{R} e^{- \\frac{1}{2\\sigma^2} |x - u_0(i)|^2 - \\lambda \\sum_{j\\in N(i)} |x - u(j)|}$ \n",
    "\n",
    "Ainsi nous avons bien $$ \\pi(u(i)|u(i^c)) \\propto e^{- \\frac{1}{2\\sigma^2} |u(i) - u_0(i)|^2 - \\lambda \\sum_{j\\in N(i)} |u(i) - u(j)|} \\quad \\blacksquare$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Espérance $\\mathbb{E}_{U \\sim \\pi}[U(i)|U(i^c) = w(i^c)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{U \\sim \\pi}[U(i)|U(i^c) = w(i^c)] = \\int_\\mathbb{R} x \\pi(x | w(i^c)) dx\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "En utilisant le résultat de la question précédente, nous obtenons:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{U \\sim \\pi}[U(i)|U(i^c) = w(i^c)] &= \\int_{R} x \\frac{\\exp(-\\frac{1}{2 \\sigma^{2}} {|x - u_{0}(i)|}^{2}) \\exp(- \\lambda \\sum_{j \\in N(i)} |x - w(j)|)}{Z} dx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "avec $Z = \\int_\\mathbb{R} e^{- \\frac{1}{2\\sigma^2} |x - u_0(i)|^2 - \\lambda \\sum_{j\\in N(i)} |x - u(j)|}$ \n",
    "\n",
    "Rappelons nous que $s = x - u_0(i)$. Nous pouvons appliquer ce changement de variable pour donner:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{U \\sim \\pi}[U(i)|U(i^c) = w(i^c)] &= \\int_{R} (x - u_0(i) + u_0(i)) \\frac{\\exp(-\\frac{1}{2 \\sigma^{2}} s^2) \\exp(- \\lambda \\sum_{j \\in N(i)} |s + u_0(i) - w(j)|)}{Z} dx \\\\\n",
    "&= u_0(i) + \\int_{R} s \\frac{\\exp(-\\frac{1}{2 \\sigma^{2}} s^2) \\exp(- \\lambda \\sum_{j \\in N(i)} |s + u_0(i) - w(j)|)}{Z} ds \\\\\n",
    "&= u_0(i) + \\frac{1}{Z} \\int_{R} s \\exp(-\\frac{1}{2 \\sigma^{2}} s^2) \\exp(- \\lambda \\sum_{j \\in N(i)} |s + u_0(i) - w(j)|) ds \\quad \\blacksquare\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Algorithme TV-ICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme présenté précedemment implémente des iterations basées en utilisant des valeurs logarithmiques qui sont ensuite normalisées et passées dans une exponentielle. Notons que la normalisation des valeurs ne change pas le résultat car elle équivaut à diviser tous les $X_k$ par un même $M$.   \n",
    "\n",
    "L'utilisation de logarithmes permet de garder la stabilité de l'algorithme en évitant d'avoir des valeurs trop élevées et permet de forcer les $X_k$ à être strictement positifs. La soustraction de $M$ à toutes les valeurs de $\\log X_k$ permet de garder des petites valeurs (entre $0$ et $1$) après l'application de la fonction exponentielle. \n",
    "\n",
    "Ce genre de manipulation peuvent s'avérer utile pour des langages de programmation de bas niveau comme C. En effet ces langages sont suceptibles au phénomènes d'overflow et d'underflow. Imaginons que nous ayons deux variable $x= 10^3$ et $y=980$ et que nous voulons calculer $\\frac{\\exp{x}}{\\exp{y}}$. Il y a de forte chance que $\\exp{x}$ provoque un overflow et donne une valeur autre que celle attendue. En soustrayant $x$ à $x$ et à $y$, on empêche d'avoir une situation d'overflow ou d'underflow lors de l'application de l'exponentielle et permet ainsi d'obtenir le résultat attendu. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. le code au-dessus des réponses aux questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Comparaison avec TV MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous comparons TV-MAP et TV-ICE avec $\\sigma=0.07$ pour 100 itérations chacuns. (cf. les images au dessus)\n",
    "\n",
    "Nous avons utilisé 3 valeurs pour $\\lambda$: 2, 20, 30 et 50. Généralement les 2 algorithmes donnents de résultats similaires.\n",
    "\n",
    "Visuellement, les résultats sont difficiles à distinguer pour $\\lambda$ égal à 2 et 20. On remarque cependant qu'à $\\lambda$ égal à 30 et 50 les 2 algorithmes donnent des images floues. On note que l'image de TV-ICE est plus floue que celle de TV-MAP. Cela indiquerait qu'à des valeurs hautes (>30) pour $\\lambda$, TV-ICE donne des images plus floue que TV-MAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = history[-1]\n",
    "\n",
    "score = []\n",
    "for image in history[:-1]:\n",
    "    score.append(np.linalg.norm(image - last))\n",
    "plt.plot(score)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Convergence of TV-ICE\")\n",
    "plt.xlabel(\"# of iterations\")\n",
    "plt.ylabel(\"Norm of Difference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme indiqué dans le papier \\[1\\], la convergence de la norme de la différence entre l'image finale de TV-ICE et les images des itérations antérieures est bien linéaire. On note que l'algorithme conver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
